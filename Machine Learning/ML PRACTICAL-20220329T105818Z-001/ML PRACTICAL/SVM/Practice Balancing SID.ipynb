{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jMv7pbdl7-Ct","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3bd33a6c-9c20-482f-9bb7-92ab0422eabf","executionInfo":{"status":"ok","timestamp":1643821265892,"user_tz":-330,"elapsed":26421,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"Hz4GiSWuo6K8"},"source":["**Importing Dataset**"]},{"cell_type":"code","execution_count":102,"metadata":{"id":"RQ9-wnlm8An_","executionInfo":{"status":"ok","timestamp":1643829061257,"user_tz":-330,"elapsed":683,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}}},"outputs":[],"source":["\n","import pandas as pd\n","#df=pd.read_excel('sample_data/Balanced Training data.xlsx')\n","#df1=pd.read_excel('sample_data/Testdata_cloth_20_ratio.xlsx')\n","df2=pd.read_csv('sample_data/Clothing_store_training_test_full.csv')\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vll2bRP6Y2vK","executionInfo":{"status":"ok","timestamp":1643821986057,"user_tz":-330,"elapsed":5227,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}},"outputId":"2d8703a6-63c8-4aa0-8d8b-c4fc49b0a5ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"9AfaxQthZBIz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aIm8PtOM8CJZ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_curve, auc"]},{"cell_type":"markdown","metadata":{"id":"0Cr6RDRTo-fe"},"source":["**Checking Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfsEFgwt8cKs"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsm8RIqS8fXE"},"outputs":[],"source":["df.shape"]},{"cell_type":"code","execution_count":110,"metadata":{"id":"8YmsbE0B8g1h","executionInfo":{"status":"ok","timestamp":1643829281669,"user_tz":-330,"elapsed":400,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}}},"outputs":[],"source":["vars = ['CLUSTYPE', 'WEB', 'AXSPEND', 'AMSPEND',#'HHKEY',\n","'ZIP_CODE',\n","'REC',\n","'FRE',\n","'MON',\n","'CC_CARD',\n","'AVRG',\n","'PC_CALC20',\n","'PSWEATERS',\n","'PKNIT_TOPS',\n","'PKNIT_DRES',\n","'PBLOUSES',\n","'PJACKETS',\n","'PCAR_PNTS',\n","'PCAS_PNTS',\n","'PSHIRTS',\n","'PDRESSES',\n","'PSUITS',\n","'POUTERWEAR',\n","'PJEWELRY',\n","'PFASHION',\n","'PLEGWEAR',\n","'sqrt sweaters',\n","'sqrt knit tops',\n","'sqrt knit dresses',\n","'sqrt blouses',\n","'sqrt jackets',\n","'sqrt career pants',\n","'sqrt casual pants',\n","'sqrt shirts',\n","'sqrt dresses',\n","'sqrt suits',\n","'sqrt outerwear',\n","'sqrt jewelry',\n","'sqrt fashion',\n","'sqrt legwear',\n","'sqrt collectibles',\n","'flag sweaters',\n","'flag knit tops',\n","'flag knit dresses',\n","'flag blouses',\n","'flag jackets',\n","'flag career pants',\n","'flag casual pants',\n","'flag shirts',\n","'flag dresses',\n","'flag suits',\n","'flag outerwear',\n","'flag jewelry',\n","'flag fashion',\n","'flag legwear',\n","'flag collectibles',\n","'sqrt spending AM']"]},{"cell_type":"code","execution_count":100,"metadata":{"id":"-0WlhbVV9Z8o","executionInfo":{"status":"ok","timestamp":1643829029723,"user_tz":-330,"elapsed":379,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}}},"outputs":[],"source":["y_cols = 'RESP'\n","x_cols = vars\n","# x_cols.remove(y_cols)"]},{"cell_type":"code","source":["y1_cols = 'RESP'\n","x1_cols = vars\n","# x_cols.remove(y_cols)"],"metadata":{"id":"TiP5X59Gvm1V","executionInfo":{"status":"ok","timestamp":1643827153690,"user_tz":-330,"elapsed":655,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b67929kG9c15"},"outputs":[],"source":["df2 = df2.replace(['Y', 'N'], [1, 0])\n","df = df.replace(['T', 'F'], [1, 0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6uu12jb9egF"},"outputs":[],"source":["df['RESP'].value_counts()"]},{"cell_type":"code","source":["df1['RESP'].value_counts()"],"metadata":{"id":"vdFe4a-Gv90y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kB1E8C2t9fv5"},"outputs":[],"source":["4762/24037"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aiMB-arf9jXX"},"outputs":[],"source":["zerodf = df[df[y_cols]== 0].sample(3841)\n","onedf = df[df[y_cols]== 1].sample(3832)\n","\n","newdf = pd.concat([zerodf, onedf], axis=0)\n","newdf[y_cols].value_counts()"]},{"cell_type":"code","source":["zerodf1 = df1[df1[y1_cols]== 0].sample(24037)\n","onedf1 = df1[df1[y1_cols]== 1].sample(4762)\n","\n","newdf1 = pd.concat([zerodf1, onedf1], axis=0)\n","newdf1[y1_cols].value_counts()"],"metadata":{"id":"J0XryHWQvuw3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mf-SsZU89k4N"},"outputs":[],"source":["df[y_cols].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kv2r0j1x9mhu"},"outputs":[],"source":["\n","newdf[y_cols].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"twvEAWyXpJZg"},"source":["**Statistical Summary of Dataset**"]},{"cell_type":"code","execution_count":103,"metadata":{"id":"rPwDOM0o9n1_","executionInfo":{"status":"ok","timestamp":1643829089315,"user_tz":-330,"elapsed":644,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}}},"outputs":[],"source":["\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn import neighbors\n","#newdf = newdf.dropna()"]},{"cell_type":"code","execution_count":93,"metadata":{"id":"4SCk3ln29pY4","executionInfo":{"status":"ok","timestamp":1643828854567,"user_tz":-330,"elapsed":625,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}}},"outputs":[],"source":["# Helper functions to calculate the performance of our models.\n","import itertools\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","from sklearn import svm, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    np.set_printoptions(precision=2)\n","    plt.figure()\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","#         print(\"Normalized confusion matrix\")\n","#     else:\n","#         print('Confusion matrix, without normalization')\n","\n","#     print(cm)\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.savefig(str(title.split('\\n')[0])+'.png')\n","    plt.show()\n","    \n","def overall_error_rate(y_pred, y_test):\n","    cnf_matrix = confusion_matrix(y_test, y_pred)\n","    fn = cnf_matrix[1,0]\n","    fp = cnf_matrix[0,1]\n","    tn = cnf_matrix[0,0]\n","    tp = cnf_matrix[1,1]\n","    n = len(y_test)\n","    return (fn+fp)/n\n","\n","def sensitivity(y_pred, y_test):\n","    cnf_matrix = confusion_matrix(y_test, y_pred)\n","    tap = pd.DataFrame(y_test).iloc[:,0].value_counts()[1]\n","    tp = cnf_matrix[1,1]\n","    return tp/tap\n","\n","def false_pos_rate(y_pred, y_test):\n","    cnf_matrix = confusion_matrix(y_test, y_pred)\n","    fp = cnf_matrix[0,1]\n","    tan = pd.DataFrame(y_test).iloc[:,0].value_counts()[0]\n","    return fp/tan\n","\n","def specificity(y_pred, y_test):\n","    cnf_matrix = confusion_matrix(y_test, y_pred)\n","    tn = cnf_matrix[0,0]\n","    tan = pd.DataFrame(y_test).iloc[:,0].value_counts()[0]\n","    return tn/tan\n","\n","def false_neg_rate(y_pred, y_test):\n","    cnf_matrix = confusion_matrix(y_test, y_pred)\n","    fn = cnf_matrix[1,0]\n","    tap = pd.DataFrame(y_test).iloc[:,0].value_counts()[1]\n","    return fn/tap\n","\n","def prop_true_pos(y_pred, y_test):\n","    cnf_matrix = confusion_matrix(y_test, y_pred)\n","    try:\n","        tpp = pd.DataFrame(y_pred).iloc[:,0].value_counts()[1]\n","    except:\n","        return 0\n","    tp = cnf_matrix[1,1]\n","    return tp/tpp\n","\n","def prop_true_neg(y_pred, y_test):\n","    cnf_matrix = confusion_matrix(y_test, y_pred)\n","    try:\n","        tn = cnf_matrix[0,0]\n","        tpn = pd.DataFrame(y_pred).iloc[:,0].value_counts()[0]\n","    except:\n","        return 0\n","    return tn/tpn\n","\n","def recall(y_pred, y_test):\n","    cnf_matrix = confusion_matrix(y_test, y_pred)\n","    try:\n","        tp = cnf_matrix[1,1]\n","        fn = cnf_matrix[1,0]\n","        tpn = pd.DataFrame(y_pred).iloc[:,0].value_counts()[0]\n","    except:\n","        return 0\n","    return tp/(fn+tp)\n","\n","def precision(y_pred, y_test):\n","    cnf_matrix = confusion_matrix(y_test, y_pred)\n","    try:\n","        tp = cnf_matrix[1,1]\n","        fp = cnf_matrix[0,1]\n","        tpn = pd.DataFrame(y_pred).iloc[:,0].value_counts()[0]\n","    except:\n","        return 0\n","    return tp/(fp+tp)\n","\n","def npv(y_pred, y_test):\n","    cnf_matrix = confusion_matrix(y_test, y_pred)\n","    try:\n","        fn = cnf_matrix[1,0]\n","        fn = cnf_matrix[1,0]\n","        tn = cnf_matrix[0,0]\n","        tpn = pd.DataFrame(y_pred).iloc[:,0].value_counts()[0]\n","    except:\n","        return 0\n","    return tn/(tn+fn)\n","\n","def f1score(y_pred, y_test):\n","    prec = precision(y_pred, y_test)\n","    rec = recall(y_pred, y_test)\n","    f1 = 2 * ((prec * rec)/(prec + rec))\n","    return f1\n","\n","def get_descriptive_data(y_pred, y_test):\n","    print(\"Accuracy: %f%%\" %(round(accuracy_score(y_test, y_pred)*100,2)))\n","    print(\"Overall Error Rate: %f%%\" %(round(overall_error_rate(y_pred, y_test)*100,2)))\n","    print('False Positive Rate: %f%%' %(round(false_pos_rate(y_pred, y_test)*100,2)))\n","    print('False Negative Rate: %f%%' %(round(false_neg_rate(y_pred, y_test)*100,2)))\n","    print('Specificity: %f%%' %(round(specificity(y_pred, y_test)*100,2)))\n","    print(\"Sensitivity: %f%%\" %(round(sensitivity(y_pred, y_test)*100,2)))\n","    print('Proportion True Positive: %f%%' %(round(prop_true_pos(y_pred, y_test)*100,2)))\n","    print('Proportion True Negative: %f%%' %(round(prop_true_neg(y_pred, y_test)*100,2)))\n","    print(\"recall: %f%%\" %(round(recall(y_pred, y_test)*100,2)))\n","    print(\"precision: %f%%\" %(round(precision(y_pred, y_test)*100,2)))\n","    print(\"FDR: %f%%\" %(100-round(precision(y_pred, y_test)*100,2)))\n","    print(\"NPV: %f%%\" %(round(precision(y_pred, y_test)*100,2)))\n","    print(\"FOR: %f%%\" %(100-round(npv(y_pred, y_test)*100,2)))\n","    print(\"F1SCORE: %f%%\" %(100-round(f1score(y_pred, y_test)*100,2)))"]},{"cell_type":"markdown","source":["Testing for Balacing"],"metadata":{"id":"WgjfaYqjX24z"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from collections import Counter\n","X_train, X_test, y_train, y_test = train_test_split(df2[x_cols].values, \n","                                                    df2[y_cols].values,\n","                                                    test_size=0.2, \n","                                                    random_state=1)\n","\n","training_df = df2.sample(frac=0.8, random_state=1)\n","test_df = df2.drop(training_df.index)\n","\n"],"metadata":{"id":"pniAbKgsX2Dv","executionInfo":{"status":"ok","timestamp":1643829230181,"user_tz":-330,"elapsed":1043,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}}},"execution_count":108,"outputs":[]},{"cell_type":"code","source":["training_df.to_csv(r'/content/sample_data/train_dataset.csv')\n","test_df.to_csv(r'/content/sample_data/test_dataset.csv')"],"metadata":{"id":"LVdOpbWhpxNd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Random Forest Balanced Dataset**"],"metadata":{"id":"gDbXBGKmm5EJ"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(df2[x_cols].values, \n","                                                   df2[y_cols].values, \n","                                                    test_size=0.2,\n","                                               random_state=1)\n","X1_train, X1_test, y1_train, y1_test = train_test_split(df2[x_cols].values, \n","                                                   df2[y_cols].values, \n","                                                  test_size=0.2,\n","                                                 random_state=100)\n","rf = RandomForestClassifier(random_state=10, n_jobs=-1)\n","rf.fit(X_train, y_train)\n","y_pred = rf.predict(X_test)\n","get_descriptive_data(y_pred, y_test)\n","y1_score = rf.predict_proba(X_test)[:, 1]\n","rf_fpr, rf_tpr, _ = roc_curve(y_test, y_score)\n","rf_roc_auc = auc(rf_fpr, rf_tpr)\n","\n","y1_pred = pd.Series(y_pred).replace([0,1],['N','Y'])\n","y1_test = pd.Series(y1_test).replace([0,1], ['N','Y'])\n","class_names = list(y1_pred.value_counts().index)\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Random Forest\\nConfusion matrix, without normalization')"],"metadata":{"id":"FpBZkXhJ6qi3","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"status":"error","timestamp":1643829290767,"user_tz":-330,"elapsed":1053,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}},"outputId":"e07ac443-441b-463e-d0d7-0e7126e3b736"},"execution_count":111,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-111-bc03829498e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                  random_state=100)\n\u001b[1;32m      9\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_descriptive_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         X, y = self._validate_data(\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         )\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'T'"]}]},{"cell_type":"markdown","metadata":{"id":"Ofy6ARFMpOPD"},"source":["**SVM**"]},{"cell_type":"markdown","metadata":{"id":"Bh1G0QVgqAr6"},"source":[""]},{"cell_type":"code","execution_count":89,"metadata":{"id":"cjN9gwj89yQM","colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"status":"error","timestamp":1643828773473,"user_tz":-330,"elapsed":387,"user":{"displayName":"Alisha Sikri","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08924703234249101438"}},"outputId":"1da11665-97f7-438f-ad6a-112ca9d9d9e7"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-89-a89c2c612090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Support Vector Machine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m X_train, X_test, y_train, y_test = train_test_split(df2[x_cols].values, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                                     \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                     random_state=1)\n\u001b[1;32m      5\u001b[0m X1_train, X1_test, y1_train, y1_test = train_test_split(df2[x_cols].values, \n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['CLUSTYPE', 'WEB', 'AXSPEND', 'AMSPEND', 'ZIP_CODE', 'REC', 'FRE',\\n       'MON', 'CC_CARD', 'AVRG', 'PC_CALC20', 'PSWEATERS', 'PKNIT_TOPS',\\n       'PKNIT_DRES', 'PBLOUSES', 'PJACKETS', 'PCAR_PNTS', 'PCAS_PNTS',\\n       'PSHIRTS', 'PDRESSES', 'PSUITS', 'POUTERWEAR', 'PJEWELRY', 'PFASHION',\\n       'PLEGWEAR'],\\n      dtype='object')] are in the [columns]\""]}],"source":["# Support Vector Machine\n","X_train, X_test, y_train, y_test = train_test_split(df2[x_cols].values, \n","                                                    df2[y_cols].values, \n","                                                    random_state=1)\n","X1_train, X1_test, y1_train, y1_test = train_test_split(df2[x_cols].values, \n","                                                    df2[y_cols].values, \n","                                                    test_size=0.2,\n","                                                    random_state=1)\n","svm = SVC(random_state=1)\n","svm.fit(X_train, y_train)\n","y_pred_svm = svm.predict(X1_test)\n","\n","y_score = svm.decision_function(X_test)\n","svm_fpr, svm_tpr, _ = roc_curve(y_test, y_score)\n","svm_roc_auc = auc(svm_fpr, svm_tpr)\n","\n","y1_pred = pd.Series(y_pred_svm).replace([0,1], ['N','Y'])\n","y1_test = pd.Series(y1_test).replace([0,1], ['N','Y'])\n","class_names = list(y1_pred.value_counts().index)\n","get_descriptive_data(y1_pred, y1_test)\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Support Vector Machine\\nConfusion matrix, without normalization')"]},{"cell_type":"markdown","metadata":{"id":"llcjp1D3qIqs"},"source":["**Logistic Regression**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Iu43GJoncyM"},"outputs":[],"source":["# Logistic Regression\n","X_train, X_test, y_train, y_test = train_test_split(df[x_cols].values, \n","                                                    df[y_cols].values, \n","                                                    random_state=1)\n","X1_train, X1_test, y1_train, y1_test = train_test_split(df2[x_cols].values, \n","                                                    df2[y_cols].values, \n","                                                    test_size=0.2,\n","                                                    random_state=1)\n","lr = LogisticRegression(random_state=1)\n","lr.fit(X_train, y_train)\n","y_pred = lr.predict(X1_test)\n","get_descriptive_data(y_pred, y1_test)\n","\n","y1_score = lr.predict_proba(X1_test)[:, 1]\n","lr_fpr, lr_tpr, _ = roc_curve(y1_test, y1_score)\n","lr_roc_auc = auc(lr_fpr, lr_tpr)\n","\n","y1_pred = pd.Series(y_pred).replace([0,1], ['N','Y'])\n","y1_test = pd.Series(y1_test).replace([0,1], ['N','Y'])\n","class_names = list(y1_pred.value_counts().index)\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Logistic Regression\\nConfusion matrix, without normalization')"]},{"cell_type":"markdown","metadata":{"id":"RbNPD2h6qRtd"},"source":["**KNN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkiRPBUc90Ul"},"outputs":[],"source":["# K-Nearest Neighbors\n","X_train, X_test, y_train, y_test = train_test_split(df[x_cols].values, \n","                                                    df[y_cols].values, \n","                                                    random_state=1)\n","X1_train, X1_test, y1_train, y1_test = train_test_split(df2[x_cols].values, \n","                                                    df2[y_cols].values, \n","                                                    test_size=0.2, \n","                                                    random_state=1)\n","knn = neighbors.KNeighborsClassifier(n_jobs=-1)\n","knn.fit(X_train, y_train)\n","\n","y_pred = knn.predict(X1_test)\n","y1_score = knn.predict_proba(X1_test)[:, 1]\n","knn_fpr, knn_tpr, _ = roc_curve(y1_test, y1_score)\n","knn_roc_auc = auc(knn_fpr, knn_tpr)\n","get_descriptive_data(y_pred, y1_test)\n","\n","y1_pred = pd.Series(y_pred).replace([0,1], ['N','Y'])\n","y1_test = pd.Series(y1_test).replace([0,1], ['N','Y'])\n","class_names = list(y1_pred.value_counts().index)\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                       title='K-Nearest Neighbors\\nConfusion matrix, without normalization')\n"]},{"cell_type":"markdown","metadata":{"id":"vX-YXhI8qrMY"},"source":["**Decision Tree**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPLJEN8IpS_z"},"outputs":[],"source":["# Decision Tree\n","X_train, X_test, y_train, y_test = train_test_split(df[x_cols].values, \n","                                                    df[y_cols].values, \n","                                                    random_state=1)\n","X1_train, X1_test, y1_train, y1_test = train_test_split(df2[x_cols].values, \n","                                                    df2[y_cols].values, \n","                                                    test_size=0.2, \n","                                                    random_state=1)\n","from sklearn.tree import DecisionTreeClassifier\n","dtc = DecisionTreeClassifier()\n","dtc.fit(X_train,y_train)\n","y_pred_dtc = dtc.predict(X1_test)\n","y1_score = dtc.predict_proba(X1_test)[:, 1]\n","dtc_fpr, dtc_tpr, _ = roc_curve(y1_test, y1_score)\n","dtc_roc_auc = auc(dtc_fpr, dtc_tpr)\n","get_descriptive_data(y_pred, y1_test)\n","\n","y1_pred = pd.Series(y_pred).replace([0,1], ['N','Y'])\n","y1_test = pd.Series(y1_test).replace([0,1], ['N','Y'])\n","class_names = list(y1_pred.value_counts().index)\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Decision Tree\\nConfusion matrix, without normalization')"]},{"cell_type":"markdown","metadata":{"id":"WWQKN3D3qyQe"},"source":["**Naive Bayes**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAdp9wAirv8B"},"outputs":[],"source":["# Naive Bayes\n","X_train, X_test, y_train, y_test = train_test_split(df[x_cols].values, \n","                                                    df[y_cols].values, \n","                                                    random_state=1)\n","X1_train, X1_test, y1_train, y1_test = train_test_split(df2[x_cols].values, \n","                                                    df2[y_cols].values, \n","                                                    test_size=0.2, \n","                                                    random_state=1)\n","from sklearn.naive_bayes import GaussianNB\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)\n","y_pred_dtc = gnb.predict(X1_test)\n","y1_score = gnb.predict_proba(X1_test)[:, 1]\n","gnb_fpr, gnb_tpr, _ = roc_curve(y1_test, y1_score)\n","gnb_roc_auc = auc(gnb_fpr, gnb_tpr)\n","get_descriptive_data(y_pred, y1_test)\n","\n","y1_pred = pd.Series(y_pred).replace([0,1], ['N','Y'])\n","y1_test = pd.Series(y1_test).replace([0,1], ['N','Y'])\n","class_names = list(y1_pred.value_counts().index)\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='Naive Bayes\\nConfusion matrix, without normalization')"]},{"cell_type":"markdown","metadata":{"id":"wxWNc8UOq5FC"},"source":["**ANN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kuiEgMEtivR"},"outputs":[],"source":["# ANN\n","X_train, X_test, y_train, y_test = train_test_split(df[x_cols], \n","                                                    df[y_cols], \n","                                                    random_state=1)\n","X1_train, X1_test, y1_train, y1_test = train_test_split(df2[x_cols].values, \n","                                                    df2[y_cols].values, \n","                                                    test_size=0.2, \n","                                                    random_state=1)\n","\n","from sklearn.neural_network import MLPClassifier\n","\n","clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), \n","                    random_state=1)\n","clf.fit(X_train, y_train)\n","MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n","            solver='lbfgs')\n","y_pred_dtc = clf.predict(X1_test)\n","y1_score = clf.predict_proba(X1_test)[:, 1]\n","clf_fpr, clf_tpr, _ = roc_curve(y1_test, y1_score)\n","clf_roc_auc = auc(clf_fpr, clf_tpr)\n","get_descriptive_data(y_pred_dtc, y1_test)\n","\n","y1_pred = pd.Series(y_pred_dtc).replace([0,1], ['N','Y'])\n","y1_test = pd.Series(y1_test).replace([0,1], ['N','Y'])\n","class_names = list(y1_pred.value_counts().index)\n","cnf_matrix = confusion_matrix(y1_test, y1_pred)\n","plot_confusion_matrix(cnf_matrix, classes=class_names,\n","                      title='NN\\nConfusion matrix, without normalization')\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Practice Balancing SID.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}